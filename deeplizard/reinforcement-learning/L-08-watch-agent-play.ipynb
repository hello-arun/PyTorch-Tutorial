{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to gym api\n",
    "\n",
    "The tutorials are taken from https://deeplizard.com/learn/video/QK_PP_2KgGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Environment\n",
    "\n",
    "Next, to create our environment, we just call `gym.make()` and pass a string of the name of the environment we want to set up. We'll be using the environment FrozenLake-v0. All the environments with their corresponding names you can use here are available on [Gym's website](https://gym.openai.com/envs/#classic_control).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Q-Table\n",
    "\n",
    "We're now going to load our trained Q-table as in `L-07-gym-Q-learning`\n",
    "\n",
    "Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using env.observation_space.n and env.action_space.n, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50757154, 0.44019976, 0.45177627, 0.45741024],\n",
       "       [0.2855845 , 0.31854987, 0.27639278, 0.42394095],\n",
       "       [0.33069364, 0.26150264, 0.26573058, 0.27020972],\n",
       "       [0.08498068, 0.15198439, 0.07541347, 0.10260243],\n",
       "       [0.52518316, 0.3934814 , 0.34749928, 0.37584832],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.16623461, 0.1321614 , 0.31466261, 0.10396448],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.37000304, 0.42302033, 0.41291809, 0.55496024],\n",
       "       [0.4862758 , 0.58800947, 0.42354015, 0.36065907],\n",
       "       [0.5296818 , 0.32909486, 0.38978523, 0.24914056],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.30754057, 0.55994573, 0.74227529, 0.49746954],\n",
       "       [0.73491478, 0.84302153, 0.76762264, 0.75333791],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.loadtxt(\"L-07-Q_table.txt\")\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch The Agent Play The Game\n",
    "This block of code is going to allow us to watch our trained agent play Frozen Lake using the knowledge it's gained from the training we completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****You fell through a hole!****\n"
     ]
    }
   ],
   "source": [
    "max_steps_per_episode = 100\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('pytorch-1.12.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bf133e68ea69dc128944cff77d5d532fddfd436c4ee2509df098c607a2dfdac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
